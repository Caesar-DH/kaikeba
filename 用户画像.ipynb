{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thinking1 如何使用用户标签来指导业务（如何提升业务）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用户生命周期的三个阶段\n",
    "\n",
    "获客：如何进行拉新，通过更精准的营销获取客户；\n",
    "\n",
    "粘客：个性化推荐，搜索排序，场景运营等；\n",
    "\n",
    "留客：流失率预测，分析关键节点降低流失率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thinking2 如果给你一堆用户数据，没有打标签。你该如何处理（如何打标签）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用聚类算法包括：K-Means，EM聚类，Mean-Shift，DBSCAN，层次聚类，PCA的方式。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thinking3 准确率和精确率有何不同（评估指标）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "准确率是对于给定的测试数据集，分类器正确分类的样本数与总样本数之比。公式是(TP+TN)/(TP+FN+FP+TN) 。\n",
    "\n",
    "精确率是针对我们预测结果而言的，它表示的是预测为正的样本中有多少是真正的正样本。公式是TP/(TP+FP)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Think4 如果你使用大众点评，想要给某个餐厅打标签。这时系统可以自动提示一些标签，你会如何设计（标签推荐）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设计成：热门餐厅，用户好评餐，热门菜品餐厅。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thinking5 我们今天使用了10种方式来解MNIST，这些方法有何不同？你还有其他方法来解决MNIST识别问题么（分类方法）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keras 是神经网络。TPOT是autoML，会列举出最优，但是耗时长。其他为传统机器学习 ，准确率也不低。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action1 针对Delicious数据集，对SimpleTagBased算法进行改进（使用NormTagBased、TagBased-TFIDF算法）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import operator\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./user_taggedbookmarks-timestamps.dat\"\n",
    "\n",
    "records = {}\n",
    "\n",
    "# 训练集，测试集\n",
    "train_data = dict()\n",
    "test_data = dict()\n",
    "# 用户标签，商品标签\n",
    "user_tags = dict()\n",
    "tag_items = dict()\n",
    "user_items = dict()\n",
    "\n",
    "# 增加\n",
    "item_tags = dict()\n",
    "tag_users = dict()\n",
    "item_users = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用测试集，计算准确率和召回率\n",
    "def precisionAndRecall(N):\n",
    "    hit = 0\n",
    "    h_recall = 0\n",
    "    h_precision = 0\n",
    "    for user,items in test_data.items():\n",
    "        if user not in train_data:\n",
    "            continue\n",
    "        # 获取Top-N推荐列表\n",
    "        rank = recommend(user, N)\n",
    "        for item,rui in rank:\n",
    "            if item in items:\n",
    "                hit = hit + 1\n",
    "        h_recall = h_recall + len(items)\n",
    "        h_precision = h_precision + N\n",
    "    # 返回准确率 和 召回率\n",
    "    return (hit/(h_precision*1.0)), (hit/(h_recall*1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对用户user推荐Top-N\n",
    "def recommend(user, N):\n",
    "    recommend_items=dict()\n",
    "    # 对Item进行打分，分数为所有的（用户对某标签使用的次数 wut, 乘以 商品被打上相同标签的次数 wti）之和\n",
    "    tagged_items = user_items[user]     \n",
    "    for tag, wut in user_tags[user].items():\n",
    "        for item, wti in tag_items[tag].items():\n",
    "            if item in tagged_items:\n",
    "                continue\n",
    "                \n",
    "            #NormTagBased、TagBased-TFIDF\n",
    "            norm = len(tag_users[tag].items())*len(user_tags[user].items()) \n",
    "            norm = math.log(len(tag_users[tag].items()) +1 )\n",
    "            \n",
    "            if item not in recommend_items:\n",
    "                recommend_items[item] = wut * wti/norm\n",
    "            else:\n",
    "                recommend_items[item] = recommend_items[item] + wut * wti /norm\n",
    "    return sorted(recommend_items.items(), key=operator.itemgetter(1), reverse=True)[0:N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用测试集，对推荐结果进行评估\n",
    "def testRecommend():\n",
    "    print(\"推荐结果评估\")\n",
    "    print(\"%3s %10s %10s\" % ('N',\"精确率\",'召回率'))\n",
    "    for n in [5,10,20,40,60,80,100]:\n",
    "        precision,recall = precisionAndRecall(n)\n",
    "        print(\"%3d %10.3f%% %10.3f%%\" % (n, precision * 100, recall * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据加载\n",
    "def load_data():\n",
    "    print(\"开始数据加载...\")\n",
    "    df = pd.read_csv(file_path, sep='\\t')\n",
    "    for i in range(len(df)):\n",
    "        uid = df['userID'][i]\n",
    "        iid = df['bookmarkID'][i]\n",
    "        tag = df['tagID'][i]\n",
    "        # 键不存在时，设置默认值{}\n",
    "        records.setdefault(uid,{})\n",
    "        records[uid].setdefault(iid,[])\n",
    "        records[uid][iid].append(tag)\n",
    "    print(\"数据集大小为 %d.\" % (len(df)))\n",
    "    print(\"设置tag的人数 %d.\" % (len(records)))\n",
    "    print(\"数据加载完成\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据集拆分为训练集和测试集\n",
    "def train_test_split(ratio, seed=100):\n",
    "    random.seed(seed)\n",
    "    for u in records.keys():\n",
    "        for i in records[u].keys():\n",
    "            # ratio比例设置为测试集\n",
    "            if random.random()<ratio:\n",
    "                test_data.setdefault(u,{})\n",
    "                test_data[u].setdefault(i,[])\n",
    "                for t in records[u][i]:\n",
    "                    test_data[u][i].append(t)\n",
    "            else:\n",
    "                train_data.setdefault(u,{})\n",
    "                train_data[u].setdefault(i,[])\n",
    "                for t in records[u][i]:\n",
    "                    train_data[u][i].append(t)        \n",
    "    print(\"训练集样本数 %d, 测试集样本数 %d\" % (len(train_data),len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置矩阵 mat[index, item] = 1\n",
    "def addValueToMat(mat, index, item, value=1):\n",
    "    if index not in mat:\n",
    "        mat.setdefault(index,{})\n",
    "        mat[index].setdefault(item,value)\n",
    "    else:\n",
    "        if item not in mat[index]:\n",
    "            mat[index][item] = value\n",
    "        else:\n",
    "            mat[index][item] += value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用训练集，初始化user_tags, tag_items, user_items\n",
    "def initStat():\n",
    "    records=train_data\n",
    "    for u,items in records.items():\n",
    "        for i,tags in items.items():\n",
    "            for tag in tags:\n",
    "                #print tag\n",
    "                # 用户和tag的关系\n",
    "                addValueToMat(user_tags, u, tag, 1)\n",
    "                # tag和item的关系\n",
    "                addValueToMat(tag_items, tag, i, 1)\n",
    "                # 用户和item的关系\n",
    "                addValueToMat(user_items, u, i, 1)\n",
    "                \n",
    "                addValueToMat(item_tags, i, tag, 1)\n",
    "                addValueToMat(tag_users, tag, u, 1)\n",
    "                addValueToMat(item_users, i, u, 1)\n",
    "    print(\"user_tags, tag_items, user_items初始化完成.\")\n",
    "    print(\"user_tags大小 %d, tag_items大小 %d, user_items大小 %d\" % (len(user_tags), len(tag_items), len(user_items)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始数据加载...\n",
      "数据集大小为 437593.\n",
      "设置tag的人数 1867.\n",
      "数据加载完成\n",
      "\n",
      "训练集样本数 1860, 测试集样本数 1793\n",
      "user_tags, tag_items, user_items初始化完成.\n",
      "user_tags大小 1860, tag_items大小 36884, user_items大小 1860\n",
      "推荐结果评估\n",
      "  N        精确率        召回率\n",
      "  5      1.008%      0.431%\n",
      " 10      0.761%      0.652%\n",
      " 20      0.549%      0.940%\n",
      " 40      0.402%      1.376%\n",
      " 60      0.328%      1.687%\n",
      " 80      0.297%      2.033%\n",
      "100      0.269%      2.306%\n"
     ]
    }
   ],
   "source": [
    "# 数据加载\n",
    "load_data()\n",
    "# 训练集，测试集拆分，20%测试集\n",
    "train_test_split(0.2)\n",
    "initStat()\n",
    "testRecommend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对Titanic数据进行清洗，建模并对乘客生存进行预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n",
      "None\n",
      "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
      "count   891.000000  891.000000  891.000000  714.000000  891.000000   \n",
      "mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n",
      "std     257.353842    0.486592    0.836071   14.526497    1.102743   \n",
      "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
      "25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n",
      "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
      "75%     668.500000    1.000000    3.000000   38.000000    1.000000   \n",
      "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
      "\n",
      "            Parch        Fare  \n",
      "count  891.000000  891.000000  \n",
      "mean     0.381594   32.204208  \n",
      "std      0.806057   49.693429  \n",
      "min      0.000000    0.000000  \n",
      "25%      0.000000    7.910400  \n",
      "50%      0.000000   14.454200  \n",
      "75%      0.000000   31.000000  \n",
      "max      6.000000  512.329200  \n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv('./train.csv',encoding='UTF-8')\n",
    "test_data = pd.read_csv('./test.csv',encoding='UTF-8')\n",
    "print(train_data.info())\n",
    "print(train_data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#平均值填充nan\n",
    "train_data['Age'].fillna(train_data['Age'].mean() , inplace= True)\n",
    "test_data['Age'].fillna(test_data['Age'].mean(),inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 票价的均值填充nan\n",
    "train_data['Fare'].fillna(train_data['Fare'].mean(), inplace=True)\n",
    "test_data['Fare'].fillna(test_data['Fare'].mean(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 登录最多的港口nan\n",
    "train_data['Embarked'].fillna('S', inplace=True)\n",
    "test_data['Embarked'].fillna('S',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征值\n",
      "     Pclass     Sex        Age  SibSp  Parch     Fare Embarked\n",
      "0         3    male  22.000000      1      0   7.2500        S\n",
      "1         1  female  38.000000      1      0  71.2833        C\n",
      "2         3  female  26.000000      0      0   7.9250        S\n",
      "3         1  female  35.000000      1      0  53.1000        S\n",
      "4         3    male  35.000000      0      0   8.0500        S\n",
      "..      ...     ...        ...    ...    ...      ...      ...\n",
      "886       2    male  27.000000      0      0  13.0000        S\n",
      "887       1  female  19.000000      0      0  30.0000        S\n",
      "888       3  female  29.699118      1      2  23.4500        S\n",
      "889       1    male  26.000000      0      0  30.0000        C\n",
      "890       3    male  32.000000      0      0   7.7500        Q\n",
      "\n",
      "[891 rows x 7 columns]\n",
      "['Age', 'Embarked=C', 'Embarked=Q', 'Embarked=S', 'Fare', 'Parch', 'Pclass', 'Sex=female', 'Sex=male', 'SibSp']\n"
     ]
    }
   ],
   "source": [
    "# 特征选择\n",
    "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
    "train_features = train_data[features]\n",
    "train_labels = train_data['Survived']\n",
    "test_features = test_data[features]\n",
    "print('特征值')\n",
    "print(train_features)\n",
    "dvec=DictVectorizer(sparse=False)\n",
    "train_features=dvec.fit_transform(train_features.to_dict(orient='record'))\n",
    "print(dvec.feature_names_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score准确率为 0.9820\n",
      "cross_val_score准确率为 0.7767\n"
     ]
    }
   ],
   "source": [
    "#使用决策树\n",
    "clf = DecisionTreeClassifier(criterion='entropy')\n",
    "clf.fit(train_features, train_labels)\n",
    "test_features=dvec.transform(test_features.to_dict(orient='record'))\n",
    "\n",
    "# 预测\n",
    "pred_labels = clf.predict(test_features)\n",
    "\n",
    "# 准确率\n",
    "acc_decision_tree = round(clf.score(train_features, train_labels), 6)\n",
    "print(u'score准确率为 %.4lf' % acc_decision_tree)\n",
    "\n",
    "# 使用K折交叉验证 统计决策树准确率\n",
    "print(u'cross_val_score准确率为 %.4lf' % np.mean(cross_val_score(clf, train_features, train_labels, cv=10)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "E:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "E:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "E:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "E:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "E:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "E:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score准确率为 0.7991\n",
      "cross_val_score准确率为 0.7969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "E:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "E:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "E:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "E:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "E:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "#LR\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(max_iter=100, verbose=True, random_state=33, tol=1e-4)\n",
    "clf.fit(train_features, train_labels)\n",
    "\n",
    "# 预测\n",
    "pred_labels = clf.predict(test_features)\n",
    "\n",
    "# 准确率\n",
    "acc_decision_tree = round(clf.score(train_features, train_labels), 6)\n",
    "print(u'score准确率为 %.4lf' % acc_decision_tree)\n",
    "\n",
    "# 使用K折交叉验证 统计决策树准确率\n",
    "print(u'cross_val_score准确率为 %.4lf' % np.mean(cross_val_score(clf, train_features, train_labels, cv=10)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: xgboost.XGBClassifier is not available and will not be used by TPOT.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Optimization Progress', max=120.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: 0.833927562613772\n",
      "Generation 2 - Current best internal CV score: 0.833927562613772\n",
      "Generation 3 - Current best internal CV score: 0.8440210909547423\n",
      "Generation 4 - Current best internal CV score: 0.8440210909547423\n",
      "Generation 5 - Current best internal CV score: 0.8440210909547423\n",
      "Best pipeline: LinearSVC(GradientBoostingClassifier(input_matrix, learning_rate=0.1, max_depth=10, max_features=0.8500000000000001, min_samples_leaf=17, min_samples_split=15, n_estimators=100, subsample=0.6000000000000001), C=1.0, dual=False, loss=squared_hinge, penalty=l2, tol=0.1)\n",
      "0.9191919191919192\n"
     ]
    }
   ],
   "source": [
    "#TPOT\n",
    "from tpot import TPOTClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "tpot = TPOTClassifier(generations=5, population_size=20, verbosity=2)\n",
    "tpot.fit(train_features, train_labels)\n",
    "print(tpot.score(train_features, train_labels))\n",
    "tpot.export('tpot_iris_pipeline.py')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
